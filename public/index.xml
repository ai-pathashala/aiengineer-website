<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>The AI Engineer</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on The AI Engineer</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Jul 2025 00:00:00 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Local model serving - Using Foundry Local</title>
      <link>http://localhost:1313/post/local-model-serving-using-foundry-local/</link>
      <pubDate>Thu, 17 Jul 2025 00:00:00 +0530</pubDate>
      <guid>http://localhost:1313/post/local-model-serving-using-foundry-local/</guid>
      <description>&lt;p&gt;In this series of articles on local model serving, we learned about using &lt;a href=&#34;https://ai-engineer.in/post/local-model-serving-using-ollama/&#34;&gt;Ollama&lt;/a&gt;, &lt;a href=&#34;https://ai-engineer.in/post/local-model-serving-using-docker-model-runner/&#34;&gt;Docker Model Runner&lt;/a&gt;, and &lt;a href=&#34;https://ai-engineer.in/post/local-model-serving-using-lmstudio/&#34;&gt;LMStudio&lt;/a&gt;. At Build 2025, &lt;a href=&#34;https://devblogs.microsoft.com/foundry/foundry-local-a-new-era-of-edge-ai/&#34;&gt;Microsoft announced Foundry Local&lt;/a&gt;. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli&#34;&gt;command-line interface&lt;/a&gt;, and offers an OpenAI API-compatible &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-rest&#34;&gt;RESTful API&lt;/a&gt;. It also supports Python, C#, Rust, and JavaScript &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-sdk&#34;&gt;SDKs for local AI model management&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Local model serving - Using Docker model runner</title>
      <link>http://localhost:1313/post/local-model-serving-using-docker-model-runner/</link>
      <pubDate>Wed, 16 Jul 2025 00:00:00 +0530</pubDate>
      <guid>http://localhost:1313/post/local-model-serving-using-docker-model-runner/</guid>
      <description>&lt;p&gt;In the local model serving landscape, we have already looked at &lt;a href=&#34;http://localhost:1313/post/local-model-serving-using-ollama/&#34;&gt;Ollama&lt;/a&gt; and &lt;a href=&#34;http://localhost:1313/post/local-model-serving-using-lmstudio/&#34;&gt;LM Studio&lt;/a&gt;. The other option I explored was &lt;a href=&#34;https://www.docker.com/blog/introducing-docker-model-runner/&#34;&gt;Docker Model Runner&lt;/a&gt; (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Local model serving - Using LM Studio</title>
      <link>http://localhost:1313/post/local-model-serving-using-lmstudio/</link>
      <pubDate>Tue, 15 Jul 2025 00:00:00 +0530</pubDate>
      <guid>http://localhost:1313/post/local-model-serving-using-lmstudio/</guid>
      <description>&lt;p&gt;In an earlier article, we looked at &lt;a href=&#34;http://localhost:1313/post/local-model-serving-using-ollama/&#34;&gt;Ollama for serving models locally&lt;/a&gt;. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today&amp;rsquo;s article, we will go over some of these features.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Local model serving - Using Ollama</title>
      <link>http://localhost:1313/post/local-model-serving-using-ollama/</link>
      <pubDate>Mon, 14 Jul 2025 00:00:00 +0530</pubDate>
      <guid>http://localhost:1313/post/local-model-serving-using-ollama/</guid>
      <description>&lt;p&gt;As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
