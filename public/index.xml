<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>The AI Engineer</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on The AI Engineer</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Dec 2025 00:00:00 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Introduction to Google Agent Development Kit</title>
      <link>http://localhost:1313/post/introduction-to-google-agent-development-kit/</link>
      <pubDate>Mon, 01 Dec 2025 00:00:00 +0530</pubDate>
      <guid>http://localhost:1313/post/introduction-to-google-agent-development-kit/</guid>
      <description>&lt;p&gt;Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Microsoft Agent Framework - Creating Azure AI Foundry Agents</title>
      <link>http://localhost:1313/post/microsoft-agent-framework-foundry-agents/</link>
      <pubDate>Wed, 08 Oct 2025 00:00:00 +0530</pubDate>
      <guid>http://localhost:1313/post/microsoft-agent-framework-foundry-agents/</guid>
      <description>&lt;p&gt;In an earlier article, we looked at a brief &lt;a href=&#34;https://ai-engineer.in/post/introduction-to-microsoft-agent-framework/&#34;&gt;introduction to the Microsoft Agent Framework&lt;/a&gt;. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and &lt;a href=&#34;https://ai-engineer.in/post/getting-started-with-foundry-agents/&#34;&gt;Microsoft Foundry Agents&lt;/a&gt; is one among those. All agent types inherit from the common base class &lt;code&gt;AIAgent&lt;/code&gt; to provide a consistent interface.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Getting Started with Foundry Agents</title>
      <link>http://localhost:1313/post/getting-started-with-foundry-agents/</link>
      <pubDate>Fri, 03 Oct 2025 00:00:00 +0530</pubDate>
      <guid>http://localhost:1313/post/getting-started-with-foundry-agents/</guid>
      <description>&lt;p&gt;Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introduction to Microsoft Agent Framework</title>
      <link>http://localhost:1313/post/introduction-to-microsoft-agent-framework/</link>
      <pubDate>Thu, 02 Oct 2025 00:00:00 +0530</pubDate>
      <guid>http://localhost:1313/post/introduction-to-microsoft-agent-framework/</guid>
      <description>&lt;p&gt;I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as &lt;a href=&#34;https://github.com/microsoft/autogen&#34;&gt;AutoGen&lt;/a&gt; and &lt;a href=&#34;https://github.com/microsoft/semantic-kernel&#34;&gt;Semantic Kernel&lt;/a&gt;. In the past, I have written &lt;a href=&#34;https://ravichaganti.com/categories/autogen/&#34;&gt;briefly about AutoGen&lt;/a&gt;. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft &lt;a href=&#34;https://devblogs.microsoft.com/foundry/introducing-microsoft-agent-framework-the-open-source-engine-for-agentic-ai-apps/&#34;&gt;announced plans to converge these frameworks&lt;/a&gt; into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Local model serving - Using Foundry Local</title>
      <link>http://localhost:1313/post/local-model-serving-using-foundry-local/</link>
      <pubDate>Thu, 17 Jul 2025 00:00:00 +0530</pubDate>
      <guid>http://localhost:1313/post/local-model-serving-using-foundry-local/</guid>
      <description>&lt;p&gt;In this series of articles on local model serving, we learned about using &lt;a href=&#34;https://ai-engineer.in/post/local-model-serving-using-ollama/&#34;&gt;Ollama&lt;/a&gt;, &lt;a href=&#34;https://ai-engineer.in/post/local-model-serving-using-docker-model-runner/&#34;&gt;Docker Model Runner&lt;/a&gt;, and &lt;a href=&#34;https://ai-engineer.in/post/local-model-serving-using-lmstudio/&#34;&gt;LMStudio&lt;/a&gt;. At Build 2025, &lt;a href=&#34;https://devblogs.microsoft.com/foundry/foundry-local-a-new-era-of-edge-ai/&#34;&gt;Microsoft announced Foundry Local&lt;/a&gt;. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli&#34;&gt;command-line interface&lt;/a&gt;, and offers an OpenAI API-compatible &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-rest&#34;&gt;RESTful API&lt;/a&gt;. It also supports Python, C#, Rust, and JavaScript &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-sdk&#34;&gt;SDKs for local AI model management&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Local model serving - Using Docker model runner</title>
      <link>http://localhost:1313/post/local-model-serving-using-docker-model-runner/</link>
      <pubDate>Wed, 16 Jul 2025 00:00:00 +0530</pubDate>
      <guid>http://localhost:1313/post/local-model-serving-using-docker-model-runner/</guid>
      <description>&lt;p&gt;In the local model serving landscape, we have already looked at &lt;a href=&#34;http://localhost:1313/post/local-model-serving-using-ollama/&#34;&gt;Ollama&lt;/a&gt; and &lt;a href=&#34;http://localhost:1313/post/local-model-serving-using-lmstudio/&#34;&gt;LM Studio&lt;/a&gt;. The other option I explored was &lt;a href=&#34;https://www.docker.com/blog/introducing-docker-model-runner/&#34;&gt;Docker Model Runner&lt;/a&gt; (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Local model serving - Using LM Studio</title>
      <link>http://localhost:1313/post/local-model-serving-using-lmstudio/</link>
      <pubDate>Tue, 15 Jul 2025 00:00:00 +0530</pubDate>
      <guid>http://localhost:1313/post/local-model-serving-using-lmstudio/</guid>
      <description>&lt;p&gt;In an earlier article, we looked at &lt;a href=&#34;http://localhost:1313/post/local-model-serving-using-ollama/&#34;&gt;Ollama for serving models locally&lt;/a&gt;. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today&amp;rsquo;s article, we will go over some of these features.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Local model serving - Using Ollama</title>
      <link>http://localhost:1313/post/local-model-serving-using-ollama/</link>
      <pubDate>Mon, 14 Jul 2025 00:00:00 +0530</pubDate>
      <guid>http://localhost:1313/post/local-model-serving-using-ollama/</guid>
      <description>&lt;p&gt;As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
