[{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for external API access, and perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nWorkflows are graph structures that connect multiple agents to perform complex, multi-step tasks.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for external API access, and perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nWorkflows are graph structures that connect multiple agents to perform complex, multi-step tasks.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for external API access, and perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nWorkflows are graph structures that connect multiple agents to perform complex, multi-step tasks.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nWorkflows are graph structures that connect multiple agents to perform complex, multi-step tasks.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nWorkflows are graph structures that connect multiple agents to perform complex, multi-step tasks.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nWorkflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is good for a predefined sequence of operations.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nWorkflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is good for a predefined sequence of operations and may include different agents, external integrations, and human interactions.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nWorkflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nWorkflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nWorkflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path leading a more deterministic behavior.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nWorkflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nWorkflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (), group chat (collaborative conversations), and magentic workflows.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nWorkflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager) workflows.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nWorkflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager coordinated) workflows.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nWorkflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager coordinated) workflows.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nWorkflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager coordinated) workflows.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nWorkflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Azure OpenAI and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Anthropic agents, Azure OpenAI and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Anthropic agents, Azure OpenAI and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a workflow.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Anthropic agents, Azure OpenAI and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Anthropic agents, Azure OpenAI and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Anthropic agents, Azure OpenAI and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) ","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn about creating agents and workflows using Microsoft Agent Framework.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn about creating agents and workflows using the Microsoft Agent Framework. We will start with diving deep into Agents and then move toward exploring workflows in-depth.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start with diving deep into Agents and then move toward exploring workflows in-depth.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents and then move toward exploring workflows in-depth.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["Microsoft Agent Framework","Agents"],"contents":"I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. Besides this, it also supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, leading to a more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n","permalink":"http://localhost:1313/post/introduction-to-microsoft-agent-framework/","tags":null,"title":"Introduction to Microsoft Agent Framework"},{"categories":["inferencing","local serving"],"contents":"In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-foundry-local/","tags":null,"title":"Local model serving - Using Foundry Local"},{"categories":["inferencing","local serving"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing","local serving"],"contents":"In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing","local serving"],"contents":"As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"}]