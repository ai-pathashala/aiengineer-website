[
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nWhat we have seen so far is an imperative way of implementing agents in ADK. We can also do the same in a declarative way using YAML configuration.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nWhat we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nWhat we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nWhat we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml ",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nWhat we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml ",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nWhat we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the \u0026ndash;type=config optional parameter to the adk create command.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nWhat we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nWhat we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\n# yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash ",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nWhat we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\n# yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash ",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nWhat we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\n# yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nWhat we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\n# yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\n# yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search ",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nWhat we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\n# yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\n# yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But, how about\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nWhat we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\n# yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\n# yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But, how about function tools such as the get_weather tool we saw earlier?\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nWhat we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\n# yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\n# yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier?\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nWhat we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\n# yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\n# yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier?\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\n# yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\n# yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier?\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\n# yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\n# yaml-language-server: $schema=https://raw.githubusercontent.com/google/adk-python/refs/heads/main/src/google/adk/agents/config_schemas/AgentConfig.json name: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier?\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier?\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier?\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the above get_weather function can be added to a tools.py module.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; ",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py ",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must add the fully-qualified path to the function.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must add the fully-qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather ",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must add the fully-qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather ",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must add the fully-qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather ",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must add the fully-qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather ",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must add the fully-qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather ",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must add the fully-qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather ",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must add the fully-qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather ",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather ",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of our way, let us start diving deep into the\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let us start diving deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive\u0026rsquo; deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "In the first article of this series on Google ADK, we looked at a quick introduction to LLM Agents in Google ADK.\n",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "In the first article of this series on Google ADK, we looked at a quick introduction to LLM Agents in Google ADK.\n",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "In the first article of this series on Google ADK, we looked at a quick introduction to LLM Agents in Google ADK. What we learned so far is just a tiny drop in the ocean. Besides offering a simple construct (Agent and LlmAgent) for defining agents, ADK provides many powerful features and integrations. In today\u0026rsquo;s article, we shall learn about customizing agent configuration.\n",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "In the first article of this series on Google ADK, we looked at a quick introduction to LLM Agents in Google ADK. What we learned so far is just a tiny drop in the ocean. Besides offering a simple construct (Agent and LlmAgent) for defining agents, ADK provides many powerful features and integrations. In today\u0026rsquo;s article, we shall learn about customizing agent configuration.\n",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "In the first article of this series on Google ADK, we looked at a quick introduction to LLM Agents in Google ADK. What we learned so far is just a tiny drop in the ocean. In addition to offering a simple construct (Agent and LlmAgent) for defining agents, ADK provides numerous powerful features and integrations. In today\u0026rsquo;s article, we shall learn about customizing agent configuration.\n",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "In the first article of this series on Google ADK, we looked at a quick introduction to LLM Agents in Google ADK. What we learned so far is just a tiny drop in the ocean. In addition to offering a simple construct (Agent and LlmAgent) for defining agents, ADK provides numerous powerful features and integrations. In today\u0026rsquo;s article, we will learn how to customize agent configuration.\n",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "In the first article of this series on Google ADK, we looked at a quick introduction to LLM Agents in Google ADK. What we learned so far is just a tiny drop in the ocean. In addition to offering a simple construct (Agent and LlmAgent) for defining agents, ADK provides numerous powerful features and integrations. In today\u0026rsquo;s article, we will learn how to customize agent configuration.\n",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "In the first article of this series on Google ADK, we looked at a quick introduction to LLM Agents in Google ADK. What we learned so far is just a tiny drop in the ocean. In addition to offering a simple construct (Agent and LlmAgent) for defining agents, ADK provides numerous powerful features and integrations. In today\u0026rsquo;s article, we will learn how to customize agent configuration.\nLLM configuration ",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "In the first article of this series on Google ADK, we looked at a quick introduction to LLM Agents in Google ADK. What we learned so far is just a tiny drop in the ocean. In addition to offering a simple construct (Agent and LlmAgent) for defining agents, ADK provides numerous powerful features and integrations. In today\u0026rsquo;s article, we will learn how to customize agent configuration.\nLLM configuration To control and customize the\n",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "In the first article of this series on Google ADK, we looked at a quick introduction to LLM Agents in Google ADK. What we learned so far is just a tiny drop in the ocean. In addition to offering a simple construct (Agent and LlmAgent) for defining agents, ADK provides numerous powerful features and integrations. In today\u0026rsquo;s article, we will learn how to customize agent configuration.\nLLM configuration To control and customize the LLM generation, model providers support settings such a temperature and safety settings.\n",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "In the first article of this series on Google ADK, we looked at a quick introduction to LLM Agents in Google ADK. What we learned so far is just a tiny drop in the ocean. In addition to offering a simple construct (Agent and LlmAgent) for defining agents, ADK provides numerous powerful features and integrations. In today\u0026rsquo;s article, we will learn how to customize agent configuration.\nLLM configuration To control and customize LLM generation, model providers support parameters such as temperature and safety. This is specified\n",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "In the first article of this series on Google ADK, we looked at a quick introduction to LLM Agents in Google ADK. What we learned so far is just a tiny drop in the ocean. In addition to offering a simple construct (Agent and LlmAgent) for defining agents, ADK provides numerous powerful features and integrations. In today\u0026rsquo;s article, we will learn how to customize agent configuration.\nLLM configuration To control and customize LLM generation, model providers support parameters such as temperature and safety. This is specified using the generate_content_config [property](Submodules - Agent Development Kit documentation) of the LlmAgent class.\n",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "In the first article of this series on Google ADK, we looked at a quick introduction to LLM Agents in Google ADK. What we learned so far is just a tiny drop in the ocean. In addition to offering a simple construct (Agent and LlmAgent) for defining agents, ADK provides numerous powerful features and integrations. In today\u0026rsquo;s article, we will learn how to customize agent configuration.\nLLM configuration To control and customize LLM generation, model providers support parameters such as temperature and safety. This is specified using the generate_content_config [property](Submodules - Agent Development Kit documentation) of the LlmAgent class.\n",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "In the first article of this series on Google ADK, we looked at a quick introduction to LLM Agents in Google ADK. What we learned so far is just a tiny drop in the ocean. In addition to offering a simple construct (Agent and LlmAgent) for defining agents, ADK provides numerous powerful features and integrations. In today\u0026rsquo;s article, we will learn how to customize agent configuration.\nLLM configuration To control and customize LLM generation, model providers support parameters such as temperature and safety. This is specified using the generate_content_config [property](Submodules - Agent Development Kit documentation) of the LlmAgent class. Let us look at an example.\n",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "In the first article of this series on Google ADK, we looked at a quick introduction to LLM Agents in Google ADK. What we learned so far is just a tiny drop in the ocean. In addition to offering a simple construct (Agent and LlmAgent) for defining agents, ADK provides numerous powerful features and integrations. In today\u0026rsquo;s article, we will learn how to customize agent configuration.\nLLM configuration To control and customize LLM generation, model providers support parameters such as temperature and safety. This is specified using the generate_content_config [property](Submodules - Agent Development Kit documentation) of the LlmAgent class. Let us look at an example.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search from google.genai import types root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;code_agent\u0026#39;, description=\u0026#39;Code agent that can generate code in any programming language.\u0026#39;, instruction=\u0026#39;You are a code agent that can generate code in any programming language.\u0026#39;, tools=[ google_search ], generate_content_config= types.GenerateContentConfig( temperature=0.7, top_p=1, frequency_penalty=0, presence_penalty=0, ) ) ",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "In the first article of this series on Google ADK, we looked at a quick introduction to LLM Agents in Google ADK. What we learned so far is just a tiny drop in the ocean. In addition to offering a simple construct (Agent and LlmAgent) for defining agents, ADK provides numerous powerful features and integrations. In today\u0026rsquo;s article, we will learn how to customize agent configuration.\nLLM configuration To control and customize LLM generation, model providers support parameters such as temperature and safety. This is specified using the generate_content_config [property](Submodules - Agent Development Kit documentation) of the LlmAgent class. Let us look at an example.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search from google.genai import types root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;code_agent\u0026#39;, description=\u0026#39;Code agent that can generate code in any programming language.\u0026#39;, instruction=\u0026#39;You are a code agent that can generate code in any programming language.\u0026#39;, tools=[ google_search ], generate_content_config= types.GenerateContentConfig( temperature=0.7, top_p=1, frequency_penalty=0, presence_penalty=0, ) ) ",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "In the first article of this series on Google ADK, we looked at a quick introduction to LLM Agents in Google ADK. What we learned so far is just a tiny drop in the ocean. In addition to offering a simple construct (Agent and LlmAgent) for defining agents, ADK provides numerous powerful features and integrations. In today\u0026rsquo;s article, we will learn how to customize agent configuration.\nLLM configuration To control and customize LLM generation, model providers support parameters such as temperature and safety. This is specified using the generate_content_config [property](Submodules - Agent Development Kit documentation) of the LlmAgent class. Let us look at an example.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search from google.genai import types root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;code_agent\u0026#39;, description=\u0026#39;Code agent that can generate code in any programming language.\u0026#39;, instruction=\u0026#39;You are a code agent that can generate code in any programming language.\u0026#39;, tools=[ google_search ], generate_content_config= types.GenerateContentConfig( temperature=0.7, top_p=1, frequency_penalty=0, presence_penalty=0, ) ) ",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "In the first article of this series on Google ADK, we looked at a quick introduction to LLM Agents in Google ADK. What we learned so far is just a tiny drop in the ocean. In addition to offering a simple construct (Agent and LlmAgent) for defining agents, ADK provides numerous powerful features and integrations. In today\u0026rsquo;s article, we will learn how to customize agent configuration.\nLLM configuration To control and customize LLM generation, model providers support parameters such as temperature and safety. This is specified using the generate_content_config [property](Submodules - Agent Development Kit documentation) of the LlmAgent class. Let us look at an example.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search from google.genai import types root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;code_agent\u0026#39;, description=\u0026#39;Code agent that can generate code in any programming language.\u0026#39;, instruction=\u0026#39;You are a code agent that can generate code in any programming language.\u0026#39;, tools=[ google_search ], generate_content_config= types.GenerateContentConfig( temperature=0.3, top_p=1, frequency_penalty=0, presence_penalty=0, ) ) ",
        "permalink": "http://localhost:1313/post/google-adk-deep-dive-into-llmagents/",
        "tags": null,
        "title": "Google ADK - Deep dive into LLM Agents"
    },
    {
        "categories": [
            "Google",
            "Agent Development Kit",
            "Agents"
        ],
        "contents": "Every other week, we see a new framework claiming to simplify AI agent development. Thanks to the growing interest in agentic AI. Earlier this year, Google released its framework for developing agents, which it called the Agent Development Kit (ADK).\nBo Yang, tech lead of the ADK project, wrote about how ADK was born and described how 100 lines of code became the most sought-after agent development framework. It is fascinating to read how ADK evolved into what it is today.\nSo, what is ADK? What makes it special when there are a million other frameworks out there? In this series of posts, we will explore answers to these questions.\nIntroduction Google ADK provides an open foundation for simplifying agent and agentic workflow development. With ADK, you can use not just Google\u0026rsquo;s Gemini models but models from any provider, making model agnostic. The agents and workflows developed using ADK are deployment-agnostic and can run on your local machine, in Google Cloud, or in your own data center. True to its developers\u0026rsquo; intentions, ADK supports interoperability with agents built in other frameworks via the Agent-to-Agent (A2A) protocol. In my work with various agent development frameworks, I found ADK to be very easy to get started with while providing powerful constructs for building enterprise-ready agents and agentic workflows. With ADK, you can create agents in Python, Go, and Java programming languages. All examples in this series of articles on ADK will be written in Python.\nLet us explore further using code.\nTo get started with creating agents, all you need is the google-adk package.\nC:\\\u0026gt; pip install google-adk C:\\\u0026gt; adk --help Usage: adk [OPTIONS] COMMAND [ARGS]... Agent Development Kit CLI tools. Options: --version Show the version and exit. --help Show this message and exit. Commands: api_server Starts a FastAPI server for agents. conformance Conformance testing tools for ADK. create Creates a new app in the current folder with prepopulated agent template. deploy Deploys agent to hosted environments. eval Evaluates an agent given the eval sets. eval_set Manage Eval Sets. run Runs an interactive CLI for a certain agent. web Starts a FastAPI server with Web UI for agents. The adk command-line provides the necessary capabilities to create, run, deploy, and evaluate agents. Using the adk web command, you can converse with the agents and debug their execution. Let us start with creating an agent using the adk create command. When you run this command, you will be prompted for a model to use, a backend for the agent deployment and services, and an API key if you have chosen the Google Gemini model.\nC:\\\u0026gt; adk create code_agent Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_API_KEY Agent created in C:\\GitHub\\google-adk-101\\code_agent: - .env - __init__.py - agent.py As seen in the output, this command generates the necessary scaffolding. The .env file contains environment variables, including GOOGLE_API_KEY for Google Gemini models. The __init__.py file serves as the agent entry point and imports the agent defined in agent.py. The agent.py contains the agent definition. Let us explore this further.\nfrom google.adk.agents.llm_agent import Agent root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;root_agent\u0026#39;, description=\u0026#39;A helpful assistant for user questions.\u0026#39;, instruction=\u0026#39;Answer user questions to the best of your knowledge\u0026#39;, ) Google ADK supports multiple agent types. The one we created is an LlmAgent. This category of agents uses an LLM as the core reasoning engine and is non-deterministic. In the preceding code, you can replace Agent with LlmAgent as both refer to the LLM Agent implementation. We will look at other types of agents later in this series.\nIn the agent definition above, the model identifies the LLM agent to use for reasoning, and the instruction specifies the agent\u0026rsquo;s behavior. This scaffold is sufficient to try running this agent.\nC:\\\u0026gt; adk run .\\code_agent\\ .... [user]: Hello, how can you help? [root_agent]: Hello! I\u0026#39;m here to help answer your questions and provide information to the best of my knowledge. Here are some examples of what I can do: * **Answer questions on a wide range of topics:** History, science, technology, culture, current events, and more. * **Explain concepts:** Describe how things work, define terms, or break down complex ideas. * **Provide summaries:** Give you a concise overview of a topic or document. * **Help with creative tasks:** Brainstorm ideas, write short pieces of text, or suggest options. * **Give recommendations:** Suggest books, movies, places, or tools based on your interests. Just tell me what you need, and I\u0026#39;ll do my best to assist you! To open the dev-ui web interface, run the adk web command.\nThe web interface provided by ADK is a great resource for debugging agent conversations and interactions with other agents and tools.\nADK provides a few built-in tools that we can use with our agents. The following example shows the built-in google-search tool for grounding LLMs with Google search results.\nfrom google.adk.agents.llm_agent import Agent from google.adk.tools import google_search root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;condition\u0026#34;}\u0026#39;, tools=[ google_search, ], ) When you run this agent using adk run, you will be presented with a conversational prompt.\nC:\\\u0026gt; adk run .\\weather_agent\\ .... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;19 ¬∞C\u0026#34;,\u0026#34;condition\u0026#34;: \u0026#34;Partly cloudy\u0026#34;} You can also supply Python functions as tools.\nfrom google.adk.agents.llm_agent import Agent import os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; root_agent = Agent( model=\u0026#39;gemini-2.5-flash\u0026#39;, name=\u0026#39;weather_agent\u0026#39;, description=\u0026#39;Weather agent that can get weather information for any location.\u0026#39;, instruction=\u0026#39;You are a weather agent that can get weather information for any location. \\ Respond in the following format: {\u0026#34;location\u0026#34;: \u0026#34;city\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;temperature\u0026#34;}\u0026#39;, tools=[ get_weather ], ) The get_weather function uses the OpenWeatherMap API to retrieve weather data for any location.\nC:\\\u0026gt; adk run .\\code_agent\\ ... Running agent weather_agent, type exit to exit. [user]: What is the weather in Bangalore like? [weather_agent]: {\u0026#34;location\u0026#34;: \u0026#34;Bangalore\u0026#34;,\u0026#34;temperature\u0026#34;: \u0026#34;18.66¬∞C\u0026#34;} Google ADK also supports the Model Context Protocol and can use other agents as tools. We will learn more about this later.\nDeclarative Configuration What we have seen so far is an imperative-based approach to implementing agents in ADK. We can also do the same declaratively using a YAML configuration. Let us look at this method.\nC:\\\u0026gt; adk create weather_agent --type=config Choose a model for the root agent: 1. gemini-2.5-flash 2. Other models (fill later) Choose model (1, 2): 1 1. Google AI 2. Vertex AI Choose a backend (1, 2): 1 Don\u0026#39;t have API Key? Create one in AI Studio: https://aistudio.google.com/apikey Enter Google API key: YOUR_GOOGLE_API_KEY Agent created in C:\\GitHub\\google-adk-101\\weather_agent: - .env - __init__.py - root_agent.yaml For creating a YAML-based agent configuration, you need to specify the --type=config optional parameter to the adk create command. This command generates root_agent.yaml instead of agent.py. If you intend to write declarative agents like this, ensure that you have root_agent.yaml always in the agent folder.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash You can add tools to this configuration.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: google_search This is how built-in tools can be specified. But what about functional tools, such as the get_weather tool we saw earlier? To use function tools within declarative configuration, we need to place them in a Python module and place that module under the agent package. For example, the get_weather function above can be added to the tools.py module.\nimport os from typing import Annotated from pydantic import Field def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; The folder structure of the agent folder will be as follows:\nC:\\\u0026gt; tree /F /A .\\weather_agent\\ Folder PATH listing Volume serial number is 987A-295A C:\\WEATHER_AGENT .env root_agent.yaml tools.py __init__.py To reference the tool in the YAML configuration, we must provide the fully qualified path to the function.\nname: root_agent description: A helpful assistant for user questions. instruction: Answer user questions to the best of your knowledge model: gemini-2.5-flash tools: - name: weather_agent.tools.get_weather With the basics out of the way, let\u0026rsquo;s dive deep into developing agents and agentic workflows with Google ADK.\n",
        "permalink": "http://localhost:1313/post/introduction-to-google-agent-development-kit/",
        "tags": null,
        "title": "Introduction to Google Agent Development Kit"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry Agents",
            "Agent Framework"
        ],
        "contents": "In an earlier article, we looked at a brief introduction to the Microsoft Agent Framework. This framework combines the best parts of AutoGen and Semantic Kernel into a unified framework for building enterprise and production-ready AI agents. It supports different types of agents, and Microsoft Foundry Agents is one among those. All agent types inherit from the common base class AIAgent to provide a consistent interface.\nThe Foundry agents are created using AzureAIAgentClient. Let us start with an example.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient( project_endpoint=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34;, model_deployment_name=\u0026#34;gpt-5-mini\u0026#34;, async_credential=credential, agent_name=\u0026#34;WeatherAgent\u0026#34;, ).create_agent( instructions=\u0026#34;You are a weather man. Provide accurate and concise weather information based on user queries.\u0026#34;, ) as agent, ): result = await agent.run(\u0026#34;What\u0026#39;s the weather like in Bengaluru?\u0026#34;) print(result.text) asyncio.run(main()) You can also supply the project_endpoint and model_deployment_name as environment variables.\nexport AZURE_AI_PROJECT_ENDPOINT=\u0026#34;https://ai-engineer-in.services.ai.azure.com/api/projects/ai-engineer-in\u0026#34; export AZURE_AI_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-4o-mini\u0026#34; This example uses the Azure CLI credentials cached on the local system to authenticate with the Foundry. The create_agent() method provides a convenient way to create an agent. The run() method on the agent instance supplies the prompt to the LLM and retrieves the generated response.\nPS C:\\\u0026gt; python .\\01-agent-basic-no-env.py I can‚Äôt access live data from here. Do you want a real‚Äëtime forecast (I can‚Äôt fetch it unless you paste it or allow a tool) or a quick summary of typical/current-season weather for Bengaluru? Quick summary (typical for early December / dry season): - Overall: mild, dry and pleasant. - Daytime highs: about 24‚Äì30¬∞C. - Night/morning lows: about 14‚Äì20¬∞C. - Rain: low chance of rain (post‚Äëmonsoon season). - Wind: light to moderate breezes. - What to wear: light layers for daytime; a light sweater/jacket for mornings/evenings. If you want current temperature, wind, humidity, or a 7‚Äëday forecast, tell me and I‚Äôll (a) explain how to get it quickly online or (b) you can paste your current weather output and I‚Äôll interpret it. Which would you like? You can generate a streaming response using the run_stream() method.\nprint(\u0026#34;Agent: \u0026#34;, end=\u0026#34;\u0026#34;, flush=True) async for chunk in agent.run_stream(\u0026#34;Tell me a short story\u0026#34;): if chunk.text: print(chunk.text, end=\u0026#34;\u0026#34;, flush=True) print() The model that we are using in these examples does not have access to real-time weather information. We can provide the agent with tools to address this. We will use the OpenWeatherMap API to retrieve the current weather at a given location and generate a response to a user\u0026rsquo;s prompt.\nimport asyncio from typing import Annotated from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from pydantic import Field from dotenv import load_dotenv import os load_dotenv() def get_weather( location: Annotated[str, Field(description=\u0026#34;The location to get the weather for.\u0026#34;)], ) -\u0026gt; str: import requests import json api_key = os.getenv(\u0026#34;OPEN_WEATHERMAP_API_KEY\u0026#34;) base_url = \u0026#34;http://api.openweathermap.org/data/2.5/weather\u0026#34; complete_url = f\u0026#34;{base_url}?q={location}\u0026amp;appid={api_key}\u0026amp;units=metric\u0026#34; response = requests.get(complete_url) data = response.json() if data[\u0026#34;cod\u0026#34;] != \u0026#34;404\u0026#34;: main_data = data[\u0026#34;main\u0026#34;] current_temperature = main_data[\u0026#34;temp\u0026#34;] return f\u0026#34;Temperature: {current_temperature}¬∞C\u0026#34; else: return \u0026#34;City not found\u0026#34; async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;WeatherAgent\u0026#34;, instructions=\u0026#34;You are a helpful weather assistant.\u0026#34;, tools=get_weather, store=True ) as agent, ): result = await agent.run(\u0026#34;Given the current climate in Bengaluru, what should I wear?\u0026#34;) print(result.text) asyncio.run(main()) When we run this, the agent uses the get_weather tool to retrieve the weather at the location specified and generates an appropriate response to the prompt.\nPS C:\\\u0026gt; python .\\03-agent-tool-call.py Right now it‚Äôs about 20.5¬∞C in Bengaluru ‚Äî mild and slightly cool. Practical dressing tips: - Base layer: a light long-sleeve shirt, cotton tee with a thin sweater, or a casual button-down. - Outer layer: carry a light jacket, hoodie, or thin cardigan ‚Äî easy to remove if it warms up. - Bottoms: jeans, chinos, or trousers are comfortable; skirts with light tights also work. - Shoes: closed shoes or sneakers; sandals are ok if you run warm, but a closed pair is more comfortable in the cool. - Accessories: a light scarf if you feel chilly in the morning/evening. Carry a compact umbrella or light windbreaker if you want to be safe against sudden showers/wind. If you tell me what you‚Äôll be doing (office, outdoor activity, evening out) or whether you‚Äôre sensitive to cold, I can suggest a more specific outfit. Want me to check rain/wind for the next few hours? Besides the function tools, such as the example above, Foundry agents also support hosted tools. This includes a code interpreter tool. The next example demonstrates this.\nimport asyncio from agent_framework import HostedCodeInterpreterTool from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential from dotenv import load_dotenv load_dotenv() async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;CodingExecutionAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant that can write and execute Python code.\u0026#34;, tools=HostedCodeInterpreterTool() ) as agent, ): result = await agent.run(\u0026#34;Calculate the 100th prime number.\u0026#34;) print(result.text) asyncio.run(main()) This program, when executed, generates and executes the code required to answer the prompt.\nPS C:\\\u0026gt; python .\\04-agent-code-interpreter.py The 100th prime number is 541. This article is a deep dive into creating Azure AI Foundry agents using the Microsoft Agent Framework. In the next parts of this series, we shall look at other types of agents that we can create using this framework.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/microsoft-agent-framework-foundry-agents/",
        "tags": null,
        "title": "Microsoft Agent Framework - Creating Azure AI Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Foundry",
            "Agents"
        ],
        "contents": "Microsoft Foundry (formerly Azure AI Foundry) is a unified system for building intelligent agents. It is a platform that provides models, tools, frameworks, and other aspects such as observability, guardrails, and enterprise-ready governance for creating AI agents. With the Foundry Agent service, developers can develop agents locally and deploy them to different environments seamlessly, leveraging the building blocks provided by Microsoft Foundry. Foundry Agent service provides the runtime that manages conversations, orchestrates tool calls, and integrates with identify and observability systems. As discussed in the previous article, agents use LLMs to reason and make decisions. Agents use tools to perform actions. Agents can participate in workflows or interact with other agents to achieve a bigger goal. This is achieved by treating agents as composable units.\nFigure 1: Agents are composable units (Source: Microsoft)\nIf you are looking to develop production-ready agents for your enterprise needs, the Foundry Agent service provides the necessary building blocks to help you achieve your goals. You can use the Foundry portal or the Foundry SDK to create agents. Before we switch gears to using the Microsoft Agent Framework, let us quickly look at using the Foundry SDK to create agents.\nFirst, let us install the necessary packages.\n$ pip install azure-ai-projects --pre $ pip install openai azure-identity python-dotenv You must create a Foundry project, deploy a model for the agent, and set the necessary environment variables.\nAZURE_AI_FOUNDRY_PROJECT_ENDPOINT=\u0026lt;Foundry-Endpoint\u0026gt; AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME=\u0026#34;gpt-5-mini\u0026#34; For this example, we will use Azure CLI to authenticate and DefaultAzureCredential to retrieve the authentication token within the Python program. Therefore, ensure that you have logged in using az login command.\nThe following example demonstrates creating an agent and chatting with the agent.\nimport os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient from azure.ai.projects.models import PromptAgentDefinition load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] foundry_model = os.environ[\u0026#34;AZURE_AI_FOUNDRY_MODEL_DEPLOYMENT_NAME\u0026#34;] instruction = \u0026#34;You are a helpful python programming tutor. Answer questions with code samples and explanation.\u0026#34; project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.create_version( agent_name=agent_name, definition=PromptAgentDefinition( model=foundry_model, instructions=instruction, ), ) openai_client = project_client.get_openai_client() conversation = openai_client.conversations.create() response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;how do I create a python function?\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) response = openai_client.responses.create( conversation=conversation.id, extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent_name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, input=\u0026#34;Build a python function that returns fibonacci sequence up to n\u0026#34;, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) AIProjectClient() creates a Foundry client instance, which can be used to create an agent. This is done using agents.create_version() method. This creates an agent in the Foundry.\nTo chat with this agent, we will need an OpenAI client. This is created using the get_openai_client() method. Using this client, we can create a conversation to use with the agent and then chat with the agent. This enables carrying multi-turn conversations.\nC:\\\u0026gt; python.exe .\\00-microsoft-foundry-agent-create-agent.py Response output: A Python function is defined with the def keyword, a name, an optional parameter list, and a block of statements. It can optionally return a value with return. Here are concise examples and explanations. 1) Basic function ```python def greet(name): \u0026#34;\u0026#34;\u0026#34;Return a greeting string for name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet(\u0026#34;Alice\u0026#34;)) # Hello, Alice! ``` .... If you want, tell me what kind of function you need and I‚Äôll show a tailored example. Response output: Here are two common interpretations of \u0026#34;Fibonacci up to n\u0026#34; and simple, clear functions for each. Both start the sequence 0, 1, 1, 2, 3, ... 1) \u0026#34;Up to n terms\u0026#34; ‚Äî return the first n Fibonacci numbers ```python from typing import List def fib_n_terms(n: int) -\u0026gt; List[int]: \u0026#34;\u0026#34;\u0026#34;Return a list of the first n Fibonacci numbers (starting with 0). fib_n_terms(0) -\u0026gt; [] fib_n_terms(1) -\u0026gt; [0] fib_n_terms(5) -\u0026gt; [0, 1, 1, 2, 3] \u0026#34;\u0026#34;\u0026#34; if n \u0026lt;= 0: return [] if n == 1: return [0] seq = [0, 1] while len(seq) \u0026lt; n: seq.append(seq[-1] + seq[-2]) return seq # Example print(fib_n_terms(6)) # [0, 1, 1, 2, 3, 5] ``` .... Notes - Time complexity for these is O(n) (n = number of terms produced or upper bound on produced terms). - Use the \u0026#34;n terms\u0026#34; function when the caller wants a fixed-length sequence; use the \u0026#34;up to value\u0026#34; or generator when limited by a maximum value. - If you prefer the sequence to start at 1, 1 instead of 0, 1, I can show that variant too. Chat with an existing agent To chat with an existing Foundry agent, we need to obtain an agent instance. This is done using project_client.agents.get(agent_name=\u0026quot;PyTutor\u0026quot;) method.\nfrom azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient import os from dotenv import load_dotenv load_dotenv() agent_name = \u0026#39;PyTutor\u0026#39; foundry_endpoint = os.environ[\u0026#34;AZURE_AI_FOUNDRY_PROJECT_ENDPOINT\u0026#34;] project_client = AIProjectClient( endpoint=foundry_endpoint, credential=DefaultAzureCredential(), ) agent = project_client.agents.get(agent_name=agent_name) print(f\u0026#34;Retrieved agent: {agent.name}\u0026#34;) openai_client = project_client.get_openai_client() # Reference the agent to get a response response = openai_client.responses.create( input=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Tell me what you can help with.\u0026#34;}], extra_body={\u0026#34;agent\u0026#34;: {\u0026#34;name\u0026#34;: agent.name, \u0026#34;type\u0026#34;: \u0026#34;agent_reference\u0026#34;}}, ) print(f\u0026#34;Response output: {response.output_text}\u0026#34;) Once we have an agent instance, we can use the OpenAI client to chat with the agent and retrieve a response.\nC:\\\u0026gt; python.exe .\\01-microsoft-foundry-agent-chat-with-existing-agent.py Retrieved agent: PyTutor Response output: I‚Äôm a Python programming tutor ‚Äî I can help with pretty much anything related to learning, writing, debugging, and improving Python code. Here‚Äôs a quick summary of what I can do and how I‚Äôll help you. What I can help with - Basics: syntax, variables, control flow, functions, classes, modules, file I/O. .... As shown earlier, agents created with the Foundry SDK are available in the Foundry Portal. You can customize the agents further using the portal functionality.\nLast updated: 7th December 2025\n",
        "permalink": "http://localhost:1313/post/getting-started-with-foundry-agents/",
        "tags": null,
        "title": "Getting Started with Foundry Agents"
    },
    {
        "categories": [
            "Microsoft",
            "Agent Framework",
            "Agents"
        ],
        "contents": "I have been following the Microsoft ecosystem for agent and agentic workflow development and have evaluated frameworks such as AutoGen and Semantic Kernel. In the past, I have written briefly about AutoGen. I used Semantic Kernel, along with Azure AI Foundry, to experiment with a few enterprise-ready agentic scenarios. Both frameworks are useful in their own ways, and Semantic Kernel, with its enterprise-deployment readiness, became the go-to framework for many. Microsoft announced plans to converge these frameworks into what it calls the Microsoft Agent Framework, which brings together the best parts of AutoGen and Semantic Kernel.\nMicrosoft Agent Framework supports enterprise integrations using Microsoft 365 Agents SDK and the enterprise-grade agent service integrations with Microsoft Foundry (Azure AI Foundry is renamed to Microsoft Foundry). Microsoft Agent Framework is built with interoperability at its core, which means it has built-in support for open standards such as the Model Context Protocol (MCP) for data and tool access and Agent-to-Agent (A2A) for inter-agent communication.\nFigure 1: Agent Framework (Source: Microsoft)\nMicrosoft Agent Framework supports Agent and Workflow orchestration. This enables LLM-driven reasoning and decision-making as well as business-logic driven deterministic agentic workflows. As a developer, you can choose what makes more sense for your use case. Agents and workflows are two primary capability categories that this framework offers.\nAgents use LLMs as their brains, tools for accessing external APIs, and to perform actions. Because of their probabilistic nature, agents are best suited to scenarios where the input task is unstructured and cannot be easily defined in advance. Microsoft Agent Framework supports different types of agents for different use cases. The Azure AI Foundry Agents are used to create agents that use the Foundry Agent service. In addition, it supports Anthropic agents, Azure OpenAI, and OpenAI chat and response agents.\nHere is a quick example of an Azure AI Foundry agent.\nimport asyncio from agent_framework.azure import AzureAIAgentClient from azure.identity.aio import AzureCliCredential async def main(): async with ( AzureCliCredential() as credential, AzureAIAgentClient(async_credential=credential).create_agent( name=\u0026#34;HelperAgent\u0026#34;, instructions=\u0026#34;You are a helpful assistant.\u0026#34; ) as agent, ): result = await agent.run(\u0026#34;Hello!\u0026#34;) print(result.text) asyncio.run(main()) Workflows are graph structures that connect multiple agents to perform complex, multi-step tasks. A workflow is well-suited to a predefined sequence of operations and may include multiple agents, external integrations, and human interactions. By defining the flow of work, a developer can control the execution path, resulting in more deterministic behavior. Microsoft Agent Framework derives its workflow orchestration capabilities from AutoGen and therefore supports sequential, concurrent (parallel), handoff (control transfer), group chat (collaborative conversations), and magentic (manager-coordinated) workflows.\nHere is a quick example of a concurrent workflow.\nfrom agent_framework.azure import AzureChatClient from agent_framework import ChatMessage, WorkflowCompletedEvent from agent_framework import ConcurrentBuilder chat_client = AzureChatClient(credential=AzureCliCredential()) researcher = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re an expert market and product researcher. Given a prompt, provide concise, factual insights,\u0026#34; \u0026#34; opportunities, and risks.\u0026#34; ), name=\u0026#34;researcher\u0026#34;, ) marketer = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a creative marketing strategist. Craft compelling value propositions and target messaging\u0026#34; \u0026#34; aligned to the prompt.\u0026#34; ), name=\u0026#34;marketer\u0026#34;, ) legal = chat_client.create_agent( instructions=( \u0026#34;You\u0026#39;re a cautious legal/compliance reviewer. Highlight constraints, disclaimers, and policy concerns\u0026#34; \u0026#34; based on the prompt.\u0026#34; ), name=\u0026#34;legal\u0026#34;, ) workflow = ConcurrentBuilder().participants([researcher, marketer, legal]).build() completion: WorkflowCompletedEvent | None = None async for event in workflow.run_stream(\u0026#34;We are launching a new budget-friendly electric bike for urban commuters.\u0026#34;): if isinstance(event, WorkflowCompletedEvent): completion = event if completion: print(\u0026#34;===== Final Aggregated Conversation (messages) =====\u0026#34;) messages: list[ChatMessage] | Any = completion.data for i, msg in enumerate(messages, start=1): name = msg.author_name if msg.author_name else \u0026#34;user\u0026#34; print(f\u0026#34;{\u0026#39;-\u0026#39; * 60}\\n\\n{i:02d} [{name}]:\\n{msg.text}\u0026#34;) In this series of articles, we will learn how to create agents and workflows using the Microsoft Agent Framework. We will start by diving deep into Agents, then move on to exploring workflows in depth.\nLast updated: 6th December 2025\n",
        "permalink": "http://localhost:1313/post/introduction-to-microsoft-agent-framework/",
        "tags": null,
        "title": "Introduction to Microsoft Agent Framework"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In this series of articles on local model serving, we learned about using Ollama, Docker Model Runner, and LMStudio. At Build 2025, Microsoft announced Foundry Local. This is specifically designed for running AI models on resource-constrained devices, making it ideal for local AI inference on edge devices. Foundry Local features model management and deployment via a command-line interface, and offers an OpenAI API-compatible RESTful API. It also supports Python, C#, Rust, and JavaScript SDKs for local AI model management.\nFoundry Local Architecture Before we delve into the details of using Foundry Local for AI inference, let\u0026rsquo;s understand its architecture and components.\nThe model management component in this architecture is responsible for managing the model lifecycle, compiling models, and maintaining the model cache. The model lifecycle includes downloading, loading, running, unloading, and deleting models from the local cache.\nThe ONNX runtime is the key component of this architecture. ONNX is a cross-platform ML model accelerator that supports models from PyTorch, JAX, and other frameworks. This runtime is responsible for executing the models and supports multiple hardware providers and device types.\nThe Foundry Local service is the OpenAI-compatible REST API interface for working with the inference engine. This REST API endpoint can be used with any programming language to interact with the inference endpoint. This service also provides the REST interface for model management.\nWith this brief overview of the Foundry Local architecture, let us now dive into the how!\nBasics Foundry Local service needs to be installed on the local Windows or macOS system. On Windows, you can use winget to install Foundry Local.\nwinget install Microsoft.FoundryLocal Once the service is installed, you can use the foundry service status command to check its status.\nPS C:\\\u0026gt; foundry service status üü¢ Model management service is running on http://127.0.0.1:11223/openai/status EP autoregistration status: Successfully downloaded and registered the following EPs: OpenVINOExecutionProvider. Valid EPs: CPUExecutionProvider, WebGpuExecutionProvider, OpenVINOExecutionProvider Foundry Local chooses a random port every time the service restarts. To avoid that, you can use the following command to configure a fixed port number.\nPS C:\\\u0026gt; foundry service set --port 22334 Saving new settings Restarting service... üî¥ Service is stopped. üü¢ Service is Started on http://127.0.0.1:22334/, PID 14444! foundry cache location command returns the model cache directory path.\nPS C:\\\u0026gt; foundry cache location üíæ Cache directory path: C:\\Users\\ravik\\.foundry\\cache\\models If you want to move the model cache to a different path, you can use the foundry service set --cachedir command and supply the new directory path as the argument.\nTo view the existing service configuration, run the foundry service set --show command.\nPS C:\\\u0026gt; foundry service set --show No settings changed { \u0026#34;defaultLogLevel\u0026#34;: 2, \u0026#34;serviceSettings\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;port\u0026#34;: 22334, \u0026#34;cacheDirectoryPath\u0026#34;: \u0026#34;C:\\\\Users\\\\ravik\\\\.foundry\\\\cache\\\\models\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;pipeName\u0026#34;: \u0026#34;inference_agent\u0026#34;, \u0026#34;defaultSecondsForModelTTL\u0026#34;: 600, \u0026#34;initialConnectionTimeoutInSeconds\u0026#34;: 6 } } To list all models available to run locally, use foundry model list command. This command lists all models available in the Foundry catalog\nThe foundry model run command runs a model for inference. This command starts an interactive session. If the model is not present in the local model cache, it gets downloaded. If you want to download the model but not run it immediately, you can use foundry model download command.\nPS C:\\\u0026gt; foundry model run phi-3.5-mini Model Phi-3.5-mini-instruct-openvino-gpu:1 was found in the local cache. Interactive Chat. Enter /? or /help for help. Press Ctrl+C to cancel generation. Type /exit to leave the chat. Interactive mode, please enter your prompt \u0026gt; In one line, What is Azure? üß† Thinking... ü§ñ Azure is Microsoft\u0026#39;fertilized cloud computing platform offering a range of cloud services, including storage, databases, AI, and virtual machines. The foundry model load command loads the model from the cache for inference.\nPS C:\\\u0026gt; foundry model load qwen2.5-0.5b üïì Loading model... üü¢ Model qwen2.5-0.5b loaded successfully By default, a model loaded this way lives for only 600 seconds. To change that, you can specify the --ttl optional parameter. You can retrieve a list of all models using the foundry service ps command.\nPS C:\\\u0026gt; foundry service ps Models running in service: Alias Model ID üü¢ qwen2.5-0.5b qwen2.5-0.5b-instruct-openvino-gpu:2 Inference API The real goal of local AI inference is to develop and use AI applications. We need to use the inference API for this. Loading a model to the foundry service enables the inference interface to the model.\nLet us first list the loaded models using the REST API.\nPS C:\\\u0026gt; (curl http://127.0.0.1:22334/v1/models).content {\u0026#34;data\u0026#34;:[{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:114688,\u0026#34;maxOutputTokens\u0026#34;:16384,\u0026#34;id\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763477048,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T14:44:08+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;},{\u0026#34;vision\u0026#34;:false,\u0026#34;toolCalling\u0026#34;:false,\u0026#34;maxInputTokens\u0026#34;:28672,\u0026#34;maxOutputTokens\u0026#34;:4096,\u0026#34;id\u0026#34;:\u0026#34;qwen2.5-0.5b-instruct-openvino-gpu:2\u0026#34;,\u0026#34;owned_by\u0026#34;:\u0026#34;Microsoft\u0026#34;,\u0026#34;permission\u0026#34;:[],\u0026#34;created\u0026#34;:1763466301,\u0026#34;CreatedTime\u0026#34;:\u0026#34;2025-11-18T11:45:01+00:00\u0026#34;,\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;model\u0026#34;}],\u0026#34;IsDelta\u0026#34;:false,\u0026#34;Successful\u0026#34;:true,\u0026#34;HttpStatusCode\u0026#34;:0,\u0026#34;object\u0026#34;:\u0026#34;list\u0026#34;} The /v1/chat/completions endpoint can be used to create a chat completion.\nPS C:\\\u0026gt; $body = @\u0026#34; \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;model\u0026#34;:\u0026#34;Phi-3.5-mini-instruct-openvino-gpu:1\u0026#34;, \u0026gt;\u0026gt; \u0026#34;messages\u0026#34;:[ \u0026gt;\u0026gt; { \u0026gt;\u0026gt; \u0026#34;role\u0026#34;:\u0026#34;user\u0026#34;, \u0026gt;\u0026gt; \u0026#34;content\u0026#34;:\u0026#34;In one line, what is Azure?\u0026#34; \u0026gt;\u0026gt; } \u0026gt;\u0026gt; ] \u0026gt;\u0026gt; } \u0026gt;\u0026gt; \u0026#34;@ PS C:\\\u0026gt; ((Invoke-WebRequest -Uri http://127.0.0.1:22334/v1/chat/completions -Method Post -ContentType \u0026#34;application/json\u0026#34; -Body $body).content | ConvertFrom-Json).Choices[0].delta.content Azure is Microsoft\u0026#39;fertility cloud computing service offering infrastructure, platforms, and services as a whole. We have got the inference working with the local model. This is a simple PowerShell command to invoke the chat completion API. You can, of course, use any OpenAI-compatible SDK in your favorite language to perform the same inference action.\nimport openai from foundry_local import FoundryLocalManager alias = \u0026#34;Phi-3.5-mini\u0026#34; manager = FoundryLocalManager(alias) client = openai.OpenAI( base_url=manager.endpoint, api_key=manager.api_key ) response = client.chat.completions.create( model=manager.get_model_info(alias).id, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one line, what is Azure?\u0026#34;}] ) print(response.choices[0].message.content) This is a quick overview of how Foundry Local can help with local AI inference on resource-constrained devices. In the later parts of this series, we will learn more about using Foundry Local models for different use cases.\nLast updated: 18th November 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-foundry-local/",
        "tags": null,
        "title": "Local model serving - Using Foundry Local"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub. Manage local models Run and interact with models both from the command line and the Docker Desktop GUI. Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; Last updated: 30th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-docker-model-runner/",
        "tags": null,
        "title": "Local model serving - Using Docker model runner"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "In an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\nLast updated: 25th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-lmstudio/",
        "tags": null,
        "title": "Local model serving - Using LM Studio"
    },
    {
        "categories": [
            "inferencing",
            "local serving"
        ],
        "contents": "As an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 2.0 GB pulling 966de95ca8a6: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.4 KB pulling fcc5a6bec9da: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 7.7 KB pulling a70ff7e570d9: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 6.0 KB pulling 56bb8bd477a5: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 96 B pulling 34bb5ab01051: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\nLast updated: 29th September 2025\n",
        "permalink": "http://localhost:1313/post/local-model-serving-using-ollama/",
        "tags": null,
        "title": "Local model serving - Using Ollama"
    }
]