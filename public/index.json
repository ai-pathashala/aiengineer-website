[{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nGetting Started You can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; ","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner.\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner. For any developer, docker is a part of the workflow.\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner. For any developer, Docker is a part of the workflow.\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner. For any developer, Docker is a part of the workflow. The Docker Model Runner makes running a local model as simple as running a container.\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner. For any developer, Docker is a part of the workflow. The Docker Model Runner makes running a local model as simple as running a container.\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner. For any developer, Docker is a part of the workflow. The Docker Model Runner makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release.\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner. For any developer, Docker is a part of the workflow. The Docker Model Runner makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release.\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release.\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40.\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally.\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub.\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub.\ndocker model pull ai/llama3.2:3B-Q4_0 ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"In the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?.\u0026#34; ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat prompt\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. If it is not enabled, for some reason, by default,\nYou can run docker model pull command to pull a model locally. You can get a list of models at Docker Hub or use Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. If it is not enabled, for some reason, by default,\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. If it is not enabled, for some reason, by default,\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. If it is not enabled, for some reason, by default, you can run docker model install-runner.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. You can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\n","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/smollm2\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Please write 500 words about the fall of Rome.\u0026#34; } ] }\u0026#39; ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"},{"categories":["book"],"contents":"It’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\nIt’s no secret that the digital industry is booming. From exciting startups to global brands, companies are reaching out to digital agencies, responding to the new possibilities available. However, the industry is fast becoming overcrowded, heaving with agencies offering similar services — on the surface, at least. Producing creative, fresh projects is the key to standing out. Unique side projects are the best place to innovate, but balancing commercially and creatively lucrative work is tricky. So, this article looks at\n","permalink":"http://localhost:1313/post/announcing-model-context-protocol/","tags":null,"title":"Announcing Mastering Model Context Protocol"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-10-reading-notes-ai-engineering-architecture-and-user-feedback/","tags":null,"title":"AI Engineering: AI engineering architecture and user feedback"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-9-reading-notes-inference-optimization/","tags":null,"title":"AI Engineering: Inference optimization"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-8-reading-notes-dataset-engineering/","tags":null,"title":"AI Engineering: Dataset engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-6-reading-notes-rag-and-agents/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-4-reading-notes-evaluate-ai-systems/","tags":null,"title":"AI Engineering: Evaluate AI systems"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-5-reading-notes-prompt-engineering/","tags":null,"title":"AI Engineering: Prompt engineering"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-3-reading-notes-evaluation-methodology/","tags":null,"title":"AI Engineering: Evaluation methodology"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-2-reading-notes-understanding-foundation-models/","tags":null,"title":"AI Engineering: Understanding foundation models"},{"categories":["book","notes"],"contents":"Chapter 1 of the book AI Engineering by Chip Huyen provides a great overview of AI\n","permalink":"http://localhost:1313/post/ai-engineering-chapter-1-reading-notes/","tags":null,"title":"AI Engineering: Introduction to Building AI Applications with Foundation Models"},{"categories":["inferencing"],"contents":"\rLast updated: 30th September 2025\nIn the local model serving landscape, we have already looked at Ollama and LM Studio. The other option I explored was Docker Model Runner (DMR). For any developer, Docker is a part of the workflow. The DMR makes running a local model as simple as running a container. This feature was introduced as a beta in the Docker Desktop 4.40 release. The key features of DMR include:\nServe models on OpenAI-compatible APIs Pull and push models to and from Docker Hub Manage local models Run and interact with models both from the command line and the Docker Desktop GUI Package model GGUF files as OCI artifacts and publish them to any container registry To get started, you can install Docker Desktop or upgrade to a version above 4.40. This installs the docker-model CLI plugin.\nPS C:\\\u0026gt; docker model Usage: docker model COMMAND Docker Model Runner Commands: df Show Docker Model Runner disk usage inspect Display detailed information on one model install-runner Install Docker Model Runner (Docker Engine only) list List the models pulled to your local environment logs Fetch the Docker Model Runner logs package Package a GGUF file into a Docker model OCI artifact, with optional licenses. ps List running models pull Pull a model from Docker Hub or HuggingFace to your local environment push Push a model to Docker Hub requests Fetch requests+responses from Docker Model Runner rm Remove local models downloaded from Docker Hub run Run a model and interact with it using a submitted prompt or chat mode status Check if the Docker Model Runner is running tag Tag a model uninstall-runner Uninstall Docker Model Runner unload Unload running models version Show the Docker Model Runner version Run \u0026#39;docker model COMMAND --help\u0026#39; for more information on a command. This feature can be enabled or disabled using docker model CLI or Docker Desktop GUI.\nYou can run docker model pull command to pull a model locally. You can obtain a list of models on Docker Hub or use the Docker Desktop GUI to browse the model catalog.\nPS C:\\\u0026gt; docker model pull ai/llama3.2:3B-Q4_0 Downloaded 1.92GB of 1.92GB Model pulled successfully The model names follow the convention {model}:{parameters}-{quantization}.\nTo generate a response, you can use the docker model run command.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 \u0026#34;In one sentence, what is a Llama?\u0026#34; A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total If you do not provide a prompt at the end of the command, an interactive chat session starts.\nPS C:\\\u0026gt; docker model run ai/llama3.2:3B-Q4_0 Interactive chat mode started. Type \u0026#39;/bye\u0026#39; to exit. \u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to alpacas, characterized by its long neck, soft fur, and distinctive ears. Token usage: 45 prompt + 34 completion = 79 total \u0026gt; /bye Chat session ended. If you enable the host-side TCP support, you can use the DMR REST API programmatically to access and interact with the model.\ncurl http://localhost:12434/engines/llama.cpp/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;ai/llama3.2:3B-Q4_0\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;In one sentence, what is a Llama?\u0026#34; } ] }\u0026#39; ","permalink":"http://localhost:1313/post/local-model-serving-using-docker-model-runner/","tags":null,"title":"Local model serving - Using Docker model runner"},{"categories":["inferencing"],"contents":"\rLast updated: 25th September 2025\nIn an earlier article, we looked at Ollama for serving models locally. While searching for various options to run models on my local machine, I came across LM Studio - Local AI, which is also available on your computer. LM Studio offers a comprehensive set of features compared to Ollama. In today\u0026rsquo;s article, we will go over some of these features.\nYou can download and install LM Studio on Windows, macOS, or Linux operating systems. When you first run LM Studio, you will be prompted to choose a user profile. I chose the developer to enable all features. You can switch between these profiles using the selection available in the taskbar of LM Studio.\nYou can use the search bar at the top of the window or the search icon in the left sidebar to download a model.\nThe best of LM Studio is the ability to change model sampling behavior.\nIn the settings dialog, under the context tab, you can set the system prompt to be used for the chat session. Under the model tab, you can set the sampling parameters, such as temperature and top-k. You can add Model Context Protocol (MCP) servers under the program tab.\nClicking on the attachment icon below the input prompt allows you to upload documents you want to include in your chat.\nTo serve a model for API access, use the Developer tab in the sidebar.\nIn the model serving settings, you can update the configuration, such as the server port, and enable serving on the local network. Once the model endpoint is running, you can access the API in your favorite programming language or simply curl!\ncurl http://localhost:1234/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama-3.2-1b-instruct\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Always answer in rhymes. Today is Thursday\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;What day is it today?\u0026#34; } ], \u0026#34;temperature\u0026#34;: 0.7, \u0026#34;max_tokens\u0026#34;: -1, \u0026#34;stream\u0026#34;: false }\u0026#39; Compared to Ollama, LM Studio offers a comprehensive set of features for an API application developer. You can experiment with different aspects of model serving as you develop your AI application. In a later post, we will look at Docker Model Runner for local model serving.\n","permalink":"http://localhost:1313/post/local-model-serving-using-lmstudio/","tags":null,"title":"Local model serving - Using LM Studio"},{"categories":["inferencing"],"contents":"\rLast updated: 29th September 2025\nAs an AI enthusiast, I always want quick access to large language models (LLMs) for experimenting with new tools and frameworks. We can always subscribe to cloud-hosted models such as OpenAI GPT or Anthropic Claude. However, these are expensive, and I prefer running a few models locally while still in the experimentation phase. Local execution also enables full customization and integration into local workflows, providing offline access and reduced latency for faster, more reliable responses. Additionally, it provides control over model updates and configurations, preventing disruptions caused by changes from cloud providers and enabling independent experimentation.\nThere are several options available for running Large Language Model (LLM) inference locally. Ollama is one such option and my favorite among all. Ollama offers access to a wide range of models and has recently enabled cloud-hosted models as well. It offers both CLI and GUI (chat interface) to interact with the loaded models.\nIn today\u0026rsquo;s article, we will learn about Ollama and explore its capabilities.\nGet started Ollama provides the easiest way to run LLMs on your local machine. Ollama is available on Windows, macOS, and Linux. Once you install Ollama, it will run as a startup process. You can use Ollama CLI to download and run models.\nPS C:\\\u0026gt; ollama Usage: ollama [flags] ollama [command] Available Commands: serve Start ollama create Create a model show Show information for a model run Run a model stop Stop a running model pull Pull a model from a registry push Push a model to a registry signin Sign in to ollama.com signout Sign out from ollama.com list List models ps List running models cp Copy a model rm Remove a model help Help about any command Flags: -h, --help help for ollama -v, --version Show version information Use \u0026#34;ollama [command] --help\u0026#34; for more information about a command. To download a model, you can use ollama pull or ollama run. The run command will load the model if it is already available in the local cache. If not, it will download the model weights.\nFor the model names, refer to the model registry.\nPS C:\\\u0026gt; ollama pull llama3.2:3b pulling manifest pulling dde5aa3fc5ff: 100% ▕███████████████▏ 2.0 GB pulling 966de95ca8a6: 100% ▕███████████████▏ 1.4 KB pulling fcc5a6bec9da: 100% ▕███████████████▏ 7.7 KB pulling a70ff7e570d9: 100% ▕███████████████▏ 6.0 KB pulling 56bb8bd477a5: 100% ▕███████████████▏ 96 B pulling 34bb5ab01051: 100% ▕███████████████▏ 561 B verifying sha256 digest writing manifest success The downloaded model can be served using the run command.\nPS C:\\\u0026gt; ollama run llama3.2:3b \u0026gt;\u0026gt;\u0026gt; In one sentence, what is a Llama? A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive long neck, soft fur, and gentle temperament. \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\\u0026gt; You can exit the chat interface by specifying /bye as the command. The model remains loaded for a few minutes (default is 5 minutes) even after you exit the chat interface.\nPS C:\\\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 2 minutes from now As shown above, in this case, the model remains loaded for an additional 2 minutes. You can keep it loaded for additional time using the --keepalive optional parameter.\nPS C:\\Users\\ravik\u0026gt; ollama run llama3.2:3b --keepalive 10m \u0026gt;\u0026gt;\u0026gt; /bye PS C:\\Users\\ravik\u0026gt; ollama ps NAME ID SIZE PROCESSOR CONTEXT UNTIL llama3.2:3b a80c4f17acd5 2.8 GB 100% CPU 4096 9 minutes from now Besides the ollama run and ollama pull commands, you can also a serve a model using the ollama serve command. This command starts a local web server and starts serving the locally available models. By default, this endpoint listens at port 11434. You can change the behavior of this command by settings the environment variables listed in the command help.\nPS C:\\\u0026gt; ollama serve -h Start ollama Usage: ollama serve [flags] Aliases: serve, start Flags: -h, --help help for serve Environment Variables: OLLAMA_DEBUG Show additional debug information (e.g. OLLAMA_DEBUG=1) OLLAMA_HOST IP Address for the ollama server (default 127.0.0.1:11434) OLLAMA_CONTEXT_LENGTH Context length to use unless otherwise specified (default: 4096) OLLAMA_KEEP_ALIVE The duration that models stay loaded in memory (default \u0026#34;5m\u0026#34;) OLLAMA_MAX_LOADED_MODELS Maximum number of loaded models per GPU OLLAMA_MAX_QUEUE Maximum number of queued requests OLLAMA_MODELS The path to the models directory OLLAMA_NUM_PARALLEL Maximum number of parallel requests OLLAMA_NOPRUNE Do not prune model blobs on startup OLLAMA_ORIGINS A comma separated list of allowed origins OLLAMA_SCHED_SPREAD Always schedule model across all GPUs OLLAMA_FLASH_ATTENTION Enabled flash attention OLLAMA_KV_CACHE_TYPE Quantization type for the K/V cache (default: f16) OLLAMA_LLM_LIBRARY Set LLM library to bypass autodetection OLLAMA_GPU_OVERHEAD Reserve a portion of VRAM per GPU (bytes) OLLAMA_LOAD_TIMEOUT How long to allow model loads to stall before giving up (default \u0026#34;5m\u0026#34;) For example, if you want to run at API endpoint at port 8080, you can run the following command.\nPS C:\\\u0026gt; $env:OLLAMA_HOST=\u0026#34;127.0.0.1:8080\u0026#34; PS C:\\\u0026gt; ollama serve The Ollama GUI runs in the background and it runs at port 11434. If you run ollama serve without setting the OLLAMA_HOST environment variable, the command fails.\nOllama GUI Ollama features a minimal GUI interface as well.\nI like this GUI and use it often to quickly chat with a few models running locally or in the cloud!\nOllama API You can programmatically interact with the model using the Ollama API. This is a typical REST API. You can use the official SDKs to abstract the complexity of dealing with the REST API directly. The Ollama API is OpenAI compatible, and therefore, the programs you write can easily be switched to use alternate OpenAI-compatible model serving APIs. The following example demonstrates the use of the Ollama Python SDK.\nfrom ollama import chat from ollama import ChatResponse response: ChatResponse = chat(model=\u0026#39;llama3.2:3b\u0026#39;, messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;In one sentence, what is a Llama?\u0026#39;, }, ]) print(response.message.content) When you run this, you will see the response from the model.\n(.venv) PS C:\\\u0026gt; python .\\01-get-started.py A llama is a domesticated mammal native to South America, closely related to camels and alpacas, known for its distinctive appearance, soft wool, and gentle temperament. Cloud models With the recent update, Ollama now supports accessing Ollama cloud-hosted models. These are especially useful when you do not have the local compute to run larger models. As of this writing, the following models are available.\nqwen3-coder:480b-cloud gpt-oss:120b-cloud gpt-oss:20b-cloud deepseek-v3.1:671b-cloud These models can be accessed at the command line as well as the cloud.\nOverall, Ollama has been my go to way of serving models locally and quick experimentation. I have tried a few more methods that I will write about in the future articles.\n","permalink":"http://localhost:1313/post/local-model-serving-using-ollama/","tags":null,"title":"Local model serving - Using Ollama"},{"categories":["elements"],"contents":"Heading example Here is example of hedings. You can use this heading by following markdownify rules. For example: use # for heading 1 and use ###### for heading 6.\nHeading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 Emphasis Emphasis, aka italics, with asterisks or underscores.\nStrong emphasis, aka bold, with asterisks or underscores.\nCombined emphasis with asterisks and underscores.\nStrikethrough uses two tildes. Scratch this.\nLink I\u0026rsquo;m an inline-style link\nI\u0026rsquo;m an inline-style link with title\nI\u0026rsquo;m a reference-style link\nI\u0026rsquo;m a relative reference to a repository file\nYou can use numbers for reference-style link definitions\nOr leave it empty and use the link text itself.\nURLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example).\nSome text to show that the reference links can follow later.\nParagraph Lorem ipsum dolor sit amet consectetur adipisicing elit. Quam nihil enim maxime corporis cumque totam aliquid nam sint inventore optio modi neque laborum officiis necessitatibus, facilis placeat pariatur! Voluptatem, sed harum pariatur adipisci voluptates voluptatum cumque, porro sint minima similique magni perferendis fuga! Optio vel ipsum excepturi tempore reiciendis id quidem? Vel in, doloribus debitis nesciunt fugit sequi magnam accusantium modi neque quis, vitae velit, pariatur harum autem a! Velit impedit atque maiores animi possimus asperiores natus repellendus excepturi sint architecto eligendi non, omnis nihil. Facilis, doloremque illum. Fugit optio laborum minus debitis natus illo perspiciatis corporis voluptatum rerum laboriosam.\nOrdered List List item List item List item List item List item Unordered List List item List item List item List item List item Notice This is a simple note.\nThis is a simple tip.\nThis is a simple info.\nTab This is first tab\rthis is second tab\rthis is third tab\rCollapse collapse 1 This is a simple collapse\rcollapse 2 This is a simple collapse\rcollapse 3 This is a simple collapse\rCode and Syntax Highlighting Inline code has back-ticks around it.\nvar s = \u0026#34;JavaScript syntax highlighting\u0026#34;; alert(s); s = \u0026#34;Python syntax highlighting\u0026#34; print s Blockquote This is a blockquote example.\nInline HTML You can also use raw HTML in your Markdown, and it\u0026rsquo;ll mostly work pretty well.\nDefinition list\rIs something people use sometimes.\rMarkdown in HTML\rDoes *not* work **very** well. Use HTML tags.\rTables Colons can be used to align columns.\nTables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u0026rsquo;t need to make the raw Markdown line up prettily. You can also use inline Markdown.\nMarkdown Less Pretty Still renders nicely 1 2 3 Image Youtube video ","permalink":"http://localhost:1313/post/elements/","tags":null,"title":"Elemets"}]